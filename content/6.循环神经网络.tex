\section{循环神经网络}

\noindent\textbf{习题6-1} 分析延时神经网络、卷积神经网络和循环神经网络的异同点。

三者都是典型的神经网络模型。
卷积神经网络是对前馈神经网络增加卷积层和池化层。
延时神经网络是对前馈神经网络增加延时器。
循环神经网络是对前馈神经网络增加自反馈的神经元。

延时神经网络和循环神经网络是给网络增加短期记忆能力的两种重要方法。

卷积神经网络和循环神经网络的区别在循环层上。
卷积神经网络没有时序性的概念，输入直接和输出挂钩；循环神经网络具有时序性，当前决策跟前一次决策有关。
举个例子，进行手写数字识别的时候，我们并不在意前一个决策结果是什么，需要用卷积神经网络；（图像识别）
而自然语言生成时，上一个词很大程度影响了下一个词，需要用循环神经网络。（自然语言处理）

\noindent\textbf{习题6-2} 推导公式（6.40）和公式（6.41）中的梯度。

\noindent\textbf{（1）6.40}

\[
\frac{\partial L_t}{\partial w_{ij}} = \sum_{k=1}^t \frac{\partial^+ z_k}{\partial w_{ij}} \frac{\partial L_t}{\partial z_k}
\]

\[
\delta_{t,k} = \frac{\partial L_t}{\partial z_k}
= \frac{\partial h_k}{\partial z_k} \frac{\partial z_{k+1}}{\partial h_k} \frac{\partial L_t}{\partial z_{k+1}}
= diag(f'(z_k))U^T \delta_{t,k+1}
\]

\[
\frac{\partial^+ z_k}{\partial w_{ij}} = [0,...,[x_k]_i,...,0]
\]

\[
\frac{\partial L_t}{\partial w_{ij}} = \sum_{k=1}^t [\delta_{t,k}]_i[x_k]_j
\]

\noindent\textbf{（2）6.41}

\[
\frac{\partial L_t}{\partial b_{ij}} = \sum_{k=1}^t \frac{\partial^+ z_k}{\partial b_{ij}} \frac{\partial L_t}{\partial z_k}
\]

\[
\delta_{t,k} = \frac{\partial L_t}{\partial z_k}
= \frac{\partial h_k}{\partial z_k} \frac{\partial z_{k+1}}{\partial h_k} \frac{\partial L_t}{\partial z_{k+1}}
= diag(f'(z_k))U^T \delta_{t,k+1}
\]

\[
\frac{\partial^+ z_k}{\partial b_{ij}} = [0,...,1,...,0]
\]

\[
\frac{\partial L_t}{\partial b_{ij}} = \sum_{k=1}^t \delta_{t,k}
\]

\noindent\textbf{习题6-3} 当使用公式$(6.50)$作为循环神经网络的状态更新公式时，分析其可能存在梯度爆炸的原因并给出解决方法。

原因：

(1) 梯度爆炸问题：令 $z_k = Uh_{k-1} + Wx_k + b$ 为在第 $k$ 时刻函数 $g(\cdot)$ 的输入，在计算公式$(6.34)$中的误差项 $\delta_{k,k} = \frac{\partial c_t}{\partial z_k}$ 时，梯度可能会过大，从而导致梯度爆炸问题。

其中$(6.34)$的误差项为$6.2$中的$\delta_{t,k}$

解决方法：
增加门控机制，例如：长短期记忆神经网络(LSTM)。


\noindent\textbf{习题6-4} 推导LSTM网络中参数的梯度，并分析其避免梯度消失的效果。

解答：（请自行观看插图） 其中E是损失函数

\noindent\textbf{习题6-5} 推导GRU网络中参数的梯度，并分析其避免梯度消失的效果。

解答：（请自行观看插图）

\noindent\textbf{习题6-6} 除了堆叠循环神经网络外，还有什么结构可以增加循环神经网络深度？

增加深度的方法是：增加同一时刻网络输入到输出之间的路径$X_t \to Y_t$ ($X_t \to H_t$或者$H_t \to Y_t$)

常见结构除了堆叠循环神经网络，还有双向循环神经网络、递归神经网络、图神经网络等。


\noindent\textbf{习题6-7} 证明当递归神经网络的结构退化为线性序列结构时，递归神经网络就等价于简单循环神经网络。

递归神经网络一般为树状的层次结构：

\[
\begin{tikzpicture}[level distance=1.5cm,
    level 1/.style={sibling distance=3cm},
    level 2/.style={sibling distance=2cm}]
    \node {y};
    \node[below] {h3} 
        child {
            node {h1}
            child {node {x1}}
            child {node {x2}}
        }
        child {
            node {h2}
            child {node {x3}}
            child {node {x4}}
        };
\end{tikzpicture}
\]

若退化为线性序列结构：

\[
\begin{tikzpicture}[level distance=1.5cm,
    level 1/.style={sibling distance=2.5cm}]
    \node {y};
    \node[below] {h3}
        child {
            node {h2}
            child {node {x3}}
        }
        child {
            node {h1}
            child {
                node {h1}
                child {node {x1}}
                child {node {x2}}
            }
        };
\end{tikzpicture}
\]

如图，即等价于简单循环网络。

（该题若不清楚，请自行查看插图，一样的内容）


