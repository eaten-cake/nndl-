\section{网络优化与正则化}

\noindent\textbf{习题7-1} 在小批量梯度下降中，试分析为什么学习率要和批量大小成正比。

\[
\theta_t = \theta_{t-1} - \frac{\alpha}{K} \sum_{(x,y)\in S_t} \frac{\partial \mathcal{L}(y, f(x, \theta))}{\partial \theta}, \frac{\alpha}{K}
\]

明显地，埃尔法和K成正比

批量大小越大，随机梯度的方差越小，引入的噪声也越小，训练也越稳定。同时可以设置较大的学习率；而批量大小较小时，需要设置较小的学习率，否则模型会不收敛，因此学习率通常随批量大小的增加而相应的增加，即成正比关系。

\noindent\textbf{习题7-2} 在Adam算法中，说明指数加权平均的偏差修正的合理性（即公式$(7.27)$和公式$(7.28)$）。

（答案请自行观看插图）

\noindent\textbf{习题7-3} 给出公式$(7.33)$和公式$(7.34)$中的函数$\psi(\cdot)$和$\phi(\cdot)$在不同优化算法中的具体形式。

（答案请自行观看插图）

\noindent\textbf{习题7-4} 证明公式$(7.43)$。

（答案请自行观看插图）

\noindent\textbf{习题7-5} 证明公式$(7.45)$。

（答案请自行观看插图）

\noindent\textbf{习题7-6} 在批量归一化中，以$f(\cdot)$取Logistic函数或ReLU函数为例，分析以下两种归一化方法的差异：$f(BN(\boldsymbol{W}\boldsymbol{a}^{(l-1)} + \boldsymbol{b}))$和$f(\boldsymbol{W}BN(\boldsymbol{a}^{(l-1)}) + \boldsymbol{b})$。（参见公式$(7.51)$）

当 BN 取 $[0,1]$ 之间时候，

1. 对于 Logistic 函数而言，$f(BN(Wa^{(l-1)} + b))$ 可以将函数的输入数值限制在 0-1 之间，避免了 Logistic 函数在输入数值过大的情况下，梯度接近于零，造成梯度消失而无法更新参数。而 $f(WBN(a^{(l-1)}) + b)$ 则因为不能将函数的输入数值限制在 0-1 之间，会出现上述情况。

2. 对于 ReLU 函数，由于当 $x>0$ 时，ReLU 梯度都是一定的，所以 ReLU 不受这种影响。

\noindent\textbf{习题7-7} 从再参数化的角度来分析批量归一化中缩放和平移的意义。（参见公式$(7.55)$）

从再参数化的角度来分析批量归一化中缩放和平移的意义

\[
\hat{z}^{(l)} = \frac{z^{(l)} - \mu_B}{\sqrt{\sigma_B^2 + \epsilon}} \odot \gamma + \beta
\]

\[
\triangleq BN_{\gamma,\beta}(z^{(l)}),
\]

在此公式中，$\gamma$ 和 $\beta$ 表示缩放和平移参数向量。

1. 通过 $\gamma$ 和 $\beta$ 能够有效控制不同的激活函数。例如：通过 $\gamma$，可以自动调整输入分布，防止 ReLU 死亡问题。

2. 有了 $\beta$ 的存在，仿射变换不再需要偏置参数。

3. 这种归一化可以提高效率，并且作为一种形式的正则化方法，提高泛化能力。

\noindent\textbf{习题7-8} 分析为什么批量归一化不能直接应用于循环神经网络。

批归一化可以应用于RNN的堆叠之间，其中归一化是"垂直"应用（即每个RNN的输出），

但是它不能"水平"应用（即在时间步之间），因为重复的rescaling会导致梯度爆炸。

主要是因为RNN梯度随时间反向计算，梯度有一个累积的过程。

\noindent\textbf{习题7-9} 证明在标准的随机梯度下降中，权重衰减正则化和$\ell_2$正则化的效果相同，并分析这一结论在动量法和Adam算法中是否依然成立。

（证明请看自行插图）

分析这一结论在动量法和Adam算法中是否成立？

L2正则化梯度更新的方向取决于最近一段时间内梯度的加权平均值。

当与自适应梯度相结合时（动量法和Adam算法），

L2正则化导致导致具有较大历史参数（和/或）梯度振幅的权重被正则化的程度小于使用权值衰减时的情况。

\noindent\textbf{习题7-10} 试分析为什么不能在循环神经网络中的循环连接上直接应用丢弃法？

当在循环神经网络上应用丢弃法，不能直接对每个时刻的隐藏状态进行随机丢弃，这样会损坏循环网络在时间维度上记忆能力。

（有点类似于7-8题，因为循环神经网络梯度计算是累加进行计算的，丢弃其中的部分，会使得梯度计算不准，即丢失记忆能力）

\noindent\textbf{习题7-11} 若使用标签平滑正则化方法，给出其交叉熵损失函数。

$$C$$ 个类，标签在第 $$s$$ 个向量上标注为 $$1-\epsilon$$

\[
L(\tilde{y}, f(x;\theta))
\]

\[
=-\tilde{y}^T log(f(x;\theta)))
\]

\[
=-\frac{\epsilon}{K-1}\sum_{c'=1,c'\neq s}^C log(f_{c'}(x;\theta))-(1-\epsilon)log(f_s(x;\theta))
\]
